# @package _global_
module:
  _target_: foundational_model.downstream_models.model.DownstreamModelLighting
  foundational_model_name: "ViTMAE"
  foundational_model_checkpoint: "./runs/happy-plasma-106/emg2qwerty_pretraining/version_None/checkpoints/epoch=3-step=12260.ckpt"
  DownstreamHead_config:
    __target__: foundational_model.downstream_models.model.DownstreamModel
    freeze_foundational_model: True
    pre_pooling_mlp_config:
      n_layers: 0
    post_pooling_mlp_config:
      n_layers: 2
      intermediate_sizes: [256]
      dropout: 0.0
    pooling_config:
      _target_: foundational_model.downstream_models.pool.AttentionPool
      n_heads: 8
      embedding_size: 1568
      attention_MLP:
        n_layers: 2
        intermediate_sizes: [512]
        dropout: 0.0
      value_MLP:
        n_layers: 2
        intermediate_sizes: [512]
        dropout: 0.0
    embedding_config:
      __target__: foundational_model.downstream_models.embedding.SimpleEmbedding
      
      
      

datamodule:
  _target_: emg2qwerty.lightning.WindowedEMGDataModule
  window_length: 8000  # 4 sec windows for 2kHz EMG
  padding: [1800, 200]  # 900ms past context, 100ms future context
